{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import validators,streamlit as st\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import YoutubeLoader, SeleniumURLLoader,UnstructuredURLLoader,PlaywrightURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NLTK download disabled. See CVE-2024-39705",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpunkt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:24\u001b[0m, in \u001b[0;36m_raise_on_nltk_download\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_on_nltk_download\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK download disabled. See CVE-2024-39705\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: NLTK download disabled. See CVE-2024-39705"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-JebO2aoKok6Saemu1kPvT3BlbkFJjIH5TAgMFat3G62EoRkH'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")   \n",
    "\n",
    "prompt_template ='''\n",
    "Provide a summary of the following text in 300 words :\n",
    "{text}\n",
    "\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ['https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaik\\.conda\\envs\\diss\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'docs.openwebui.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.openwebui.com/'}, page_content='Open WebUI\\n\\nSponsored by Open WebUI\\n\\nThe top banner spot is reserved for only two Enterprise Diamond sponsors on a first-come, first-served basis\\n\\nOpen WebUI is an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. It supports various LLM runners, including Ollama and OpenAI-compatible APIs.\\n\\nImportant Note on User Roles and Privacy:\\n\\nAdmin Creation: The first account created on Open WebUI gains Administrator privileges, controlling user management and system settings.\\n\\nUser Registrations: Subsequent sign-ups start with Pending status, requiring Administrator approval for access.\\n\\nPrivacy and Data Security: All your data, including login details, is locally stored on your device. Open WebUI ensures strict confidentiality and no external requests for enhanced privacy and security.\\n\\nQuick Start with Docker üê≥ (Recommended)\\u200b\\n\\ntip\\n\\nDisabling Login for Single User\\u200b\\n\\nIf you want to disable login for a single-user setup, set WEBUI_AUTH to False. This will bypass the login page.\\n\\nwarning\\n\\nYou cannot switch between single-user mode and multi-account mode after this change.\\n\\ndanger\\n\\nWhen using Docker to install Open WebUI, make sure to include the -v open-webui:/app/backend/data in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\\n\\nInstallation with Default Configuration\\u200b\\n\\nIf Ollama is on your computer, use this command:\\n\\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\\n\\nIf Ollama is on a Different Server, use this command:\\n\\nTo connect to Ollama on another server, change the OLLAMA_BASE_URL to the server\\'s URL:\\n\\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\\n\\nTo run Open WebUI with Nvidia GPU support, use this command:\\n\\ndocker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\\n\\nInstallation for OpenAI API Usage Only\\u200b\\n\\nIf you\\'re only using OpenAI API, use this command:\\n\\ndocker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\\n\\nInstalling Open WebUI with Bundled Ollama Support\\u200b\\n\\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\\n\\nWith GPU Support: Utilize GPU resources by running the following command:\\n\\ndocker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\\n\\nFor CPU Only: If you\\'re not using a GPU, use this command instead:\\n\\ndocker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\\n\\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\\n\\nAfter installation, you can access Open WebUI at http://localhost:3000. Enjoy! üòÑ\\n\\nUsing the Dev Branch üåô\\u200b\\n\\nwarning\\n\\nThe :dev branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\\n\\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the :dev tag like this:\\n\\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:dev\\n\\nManual Installation\\u200b\\n\\nInstallation with pip (Beta)\\u200b\\n\\nFor users who prefer to use Python\\'s package manager pip, Open WebUI offers a installation method. Python 3.11 is required for this method.\\n\\nInstall Open WebUI: Open your terminal and run the following command:\\n\\npip install open-webui\\n\\nStart Open WebUI: Once installed, start the server using:\\n\\nopen-webui serve\\n\\nThis method installs all necessary dependencies and starts Open WebUI, allowing for a simple and efficient setup. After installation, you can access Open WebUI at http://localhost:8080. Enjoy! üòÑ\\n\\nOther Installation Methods\\u200b\\n\\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our Open WebUI Documentation or join our Discord community for comprehensive guidance.\\n\\nTroubleshooting\\u200b\\n\\nIf you\\'re facing various issues like \"Open WebUI: Server Connection Error\", see TROUBLESHOOTING for information on how to troubleshoot and/or join our Open WebUI Discord community.\\n\\nUpdating\\u200b\\n\\nCheck out our full updating guide.\\n\\nIn case you want to update your local Docker installation to the latest version, you can do it with Watchtower:\\n\\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\\n\\nIn the last part of the command, replace open-webui with your container name if it is different.\\n\\nContinue with the full getting started guide.\\n\\nSponsors üôå\\u200b\\n\\nWe are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!\\n\\nOpen WebUI\\n\\nOn a mission to build the best open-source AI user interface.\\n\\nEdit this page\\n\\nQuick Start with Docker üê≥ (Recommended)\\n\\nInstallation with Default Configuration\\n\\nInstallation for OpenAI API Usage Only\\n\\nInstalling Open WebUI with Bundled Ollama Support\\n\\nUsing the Dev Branch üåô\\n\\nManual Installation\\n\\nInstallation with pip (Beta)\\n\\nOther Installation Methods\\n\\nTroubleshooting\\n\\nUpdating\\n\\nSponsors üôå')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = loader = UnstructuredURLLoader(urls=['https://docs.openwebui.com/'],ssl_verify=False,headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"})\n",
    "data = loader.load()\n",
    "data\n",
    "\n",
    "                # Debug: Show loaded content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NLTK download disabled. See CVE-2024-39705",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpunkt_tab\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mshaik/nltk_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\diss\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:24\u001b[0m, in \u001b[0;36m_raise_on_nltk_download\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_on_nltk_download\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK download disabled. See CVE-2024-39705\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: NLTK download disabled. See CVE-2024-39705"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab', download_dir='C:\\\\Users\\\\shaik/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m loader \u001b[38;5;241m=\u001b[39m PlaywrightURLLoader(urls\u001b[38;5;241m=\u001b[39murl,)\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\document_loaders\\base.py:30\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\document_loaders\\url_playwright.py:171\u001b[0m, in \u001b[0;36mPlaywrightURLLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the specified URLs using Playwright and create Document instances.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    A list of Document instances with loaded content.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplaywright\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_playwright\n\u001b[1;32m--> 171\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheadless\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murls\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\diss\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "loader = PlaywrightURLLoader(urls=url,)\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def my_tool(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        content = await page.inner_text()\n",
    "        await browser.close()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object my_tool at 0x00000183B854BB40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaik\\AppData\\Local\\Temp\\ipykernel_29160\\1305899675.py:1: RuntimeWarning: coroutine 'my_tool' was never awaited\n",
      "  print(my_tool(url))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "print(my_tool(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models'}, page_content='')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching or processing ['https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models'], exception: Message: invalid argument: 'url' must be a string\n",
      "  (Session info: chrome-headless-shell=126.0.6478.185)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF74A59EEB2+31554]\n",
      "\t(No symbol) [0x00007FF74A517EE9]\n",
      "\t(No symbol) [0x00007FF74A3D872A]\n",
      "\t(No symbol) [0x00007FF74A46D404]\n",
      "\t(No symbol) [0x00007FF74A44D02A]\n",
      "\t(No symbol) [0x00007FF74A46C977]\n",
      "\t(No symbol) [0x00007FF74A44CDD3]\n",
      "\t(No symbol) [0x00007FF74A41A33B]\n",
      "\t(No symbol) [0x00007FF74A41AED1]\n",
      "\tGetHandleVerifier [0x00007FF74A8A8B2D+3217341]\n",
      "\tGetHandleVerifier [0x00007FF74A8F5AF3+3532675]\n",
      "\tGetHandleVerifier [0x00007FF74A8EB0F0+3489152]\n",
      "\tGetHandleVerifier [0x00007FF74A64E786+750614]\n",
      "\t(No symbol) [0x00007FF74A52376F]\n",
      "\t(No symbol) [0x00007FF74A51EB24]\n",
      "\t(No symbol) [0x00007FF74A51ECB2]\n",
      "\t(No symbol) [0x00007FF74A50E17F]\n",
      "\tBaseThreadInitThunk [0x00007FFB1BA2257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFB1CACAF28+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader = SeleniumURLLoader(urls=[url])\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models', 'title': 'üìù Text Summarization with Large Language Models | Kaggle', 'description': 'Explore and run machine learning code with Kaggle Notebooks | Using data from Samsum Dataset Text summarization', 'language': 'en'}, page_content='')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =\" \"\n",
    "for page in data:\n",
    "    text +=page.page_content+ \" \"\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:64\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n",
      "\u001b[1;31mValueError\u001b[0m: The path is not a valid file: path/to/chromedriver",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m options\u001b[38;5;241m.\u001b[39mheadless \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/chromedriver\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Update with your path to chromedriver\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_page_content\u001b[39m(url):\n\u001b[0;32m     12\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[0;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:78\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     77\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Setup Selenium\n",
    "options = Options()\n",
    "options.headless = True\n",
    "service = Service('path/to/chromedriver')  # Update with your path to chromedriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    driver.get(url)\n",
    "    # Wait for page to fully load if needed\n",
    "    driver.implicitly_wait(10)  # Adjust as necessary\n",
    "    page_content = driver.page_source\n",
    "    driver.quit()\n",
    "    return page_content\n",
    "\n",
    "url = \"https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models\"\n",
    "content = fetch_page_content(url)\n",
    "print(content[:1000])  # Print first 1000 characters for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models', 'title': 'üìù Text Summarization with Large Language Models | Kaggle', 'description': 'Explore and run machine learning code with Kaggle Notebooks | Using data from Samsum Dataset Text summarization', 'language': 'en'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in data:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content[:1000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content\n\u001b[0;32m     13\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_page_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(content[:\u001b[38;5;241m1000\u001b[39m])  \u001b[38;5;66;03m# Print first 1000 characters for inspection\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m, in \u001b[0;36mfetch_page_content\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_page_content\u001b[39m(url):\n\u001b[1;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchromium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        page.goto(url)\n",
    "        page.wait_for_load_state('networkidle')  # Wait until the page is fully loaded\n",
    "        content = page.content()\n",
    "        browser.close()\n",
    "    return content\n",
    "\n",
    "url = \"https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models\"\n",
    "content = fetch_page_content(url)\n",
    "print(content[:1000])  # Print first 1000 characters for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3' coro=<Connection.run() done, defined at c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\playwright\\_impl\\_connection.py:265> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 272, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\base_events.py\", line 503, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py:131: RuntimeWarning: coroutine 'fetch_data' was never awaited\n",
      "  new_pieces.append((start, end))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Run the asynchronous main function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[45], line 16\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     15\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fetch_page_content(url)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(content[:\u001b[38;5;241m1000\u001b[39m])\n",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m, in \u001b[0;36mfetch_page_content\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_page_content\u001b[39m(url):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m      6\u001b[0m         browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m         page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mnew_page()\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[0m, in \u001b[0;36mPipeTransport.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         startupinfo\u001b[38;5;241m.\u001b[39mwShowWindow \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSW_HIDE\n\u001b[0;32m    119\u001b[0m     executable_path, entrypoint_path \u001b[38;5;241m=\u001b[39m compute_driver_executable()\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m    121\u001b[0m         executable_path,\n\u001b[0;32m    122\u001b[0m         entrypoint_path,\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m         stdin\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    125\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    126\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m_get_stderr_fileno(),\n\u001b[0;32m    127\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,\n\u001b[0;32m    128\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    129\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstartupinfo,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_error_future\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\subprocess.py:223\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m    221\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    222\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 223\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    224\u001b[0m     protocol_factory,\n\u001b[0;32m    225\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    226\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    227\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\base_events.py:1708\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1706\u001b[0m     debug_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1708\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1709\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1710\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1712\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, debug_log, transport)\n",
      "File \u001b[1;32mc:\\Users\\shaik\\.conda\\envs\\hugrag\\Lib\\asyncio\\base_events.py:503\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[0;32m    500\u001b[0m                                      stdin, stdout, stderr, bufsize,\n\u001b[0;32m    501\u001b[0m                                      extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def fetch_page_content(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        await page.wait_for_load_state('networkidle')  # Ensure all content is loaded\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "    return content\n",
    "\n",
    "async def main():\n",
    "    url = \"https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models\"\n",
    "    content = await fetch_page_content(url)\n",
    "    print(content[:1000])  # Print the first 1000 characters for inspection\n",
    "\n",
    "# Run the asynchronous main function\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=' 8\\tText Summarization Models\\n8.1\\tFacebook\\'s BART-large\\nThe BART-large model, developed by Facebook AI, is a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture tailored for text summarization tasks. BART is a powerful sequence-to-sequence model that combines the strengths of both BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer). BART is designed to handle various NLP tasks by reconstructing original texts from corrupted input sequences. This ability makes it versatile in a range of applications, including text generation, translation, and particularly summarization.\\n8.1.1\\tModel Architecture\\nThe BART-large model is based on the standard BART architecture, which consists of a Transformer-based encoder-decoder framework. The encoder is responsible for reading the input text and converting it into a sequence of representations, while the decoder generates the output text (in this case, the summary) based on these representations.\\nEncoder: The encoder in BART-large operates in a bidirectional manner, similar to BERT. It processes the entire input sequence, taking into account the context from both the left and right sides of each token. This enables the model to capture the full context of the input text, which is crucial for producing accurate summaries.\\nDecoder: The decoder in BART is autoregressive, like GPT, meaning it generates the summary token by token, with each token being conditioned on the previously generated ones. This allows the model to generate coherent and fluent summaries that are consistent with the input text.\\nPretraining and Fine-tuning: BART-large is pretrained on a large corpus of text data using a denoising objective, where the model learns to reconstruct the original text from a corrupted version. After pretraining, the model is fine-tuned specifically on the CNN/DailyMail dataset, which is a popular benchmark for summarization tasks. The fine-tuning process tailors the model\\'s capabilities to generate high-quality summaries for news articles.\\n\\n8.1.2\\tKey Features and Performance\\n\\nText Summarization: The BART-large model excels in abstractive summarization, where the summary is generated in natural language, potentially using new phrases that are not present in the original text. This makes it more flexible and capable of producing summaries that are concise yet informative.\\n8.1.2.1\\tROUGE Scores \\nThe performance of BART-large is often measured using ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, which compare the overlap between the generated summary and reference summaries. BART-large achieves high ROUGE scores on the CNN/DailyMail dataset, making it one of the top-performing models in the field of summarization.\\n8.1.2.2\\tModel Size\\n As a \"large\" model, BART-large has 12 encoder layers and 12 decoder layers, with a total of 406 million parameters. This large parameter count allows the model to capture complex patterns in the data, contributing to its strong performance but also requiring significant computational resources for training and inference.\\n8.1.3\\tApplications\\nBART-large is widely used for generating summaries of long documents, news articles, and reports. Its ability to produce coherent and concise summaries has made it a popular choice in both academic research and industry applications. It is particularly useful in scenarios where quick and accurate summarization of text is needed, such as in news aggregation services, legal document analysis, and content curation platforms.\\n8.1.4\\tConclusion\\nFacebook\\'s BART-large is a state-of-the-art model for abstractive text summarization, leveraging a sophisticated encoder-decoder architecture to produce high-quality summaries. Its effectiveness in capturing the essence of lengthy documents makes it a valuable tool in various natural language processing applications.\\n\\n\\n\\n8.2\\tGoogle‚Äôs Pegasus\\nThe Pegasus model, developed by Google Research, is a specialized variant of the Pegasus (Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence) architecture, tailored for the task of abstractive text summarization. Pegasus is designed to excel at generating concise and coherent summaries by pretraining the model with a unique objective that prepares it for summarization tasks. The Pegasus model is specifically fine-tuned on the XSum (Extreme Summarization) dataset, which is known for its challenging and concise summaries, often just a single sentence long.\\n', metadata={'source': 'https://example.com'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain \n",
    "from langchain_openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "docs=''' 8\tText Summarization Models\n",
    "8.1\tFacebook's BART-large\n",
    "The BART-large model, developed by Facebook AI, is a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture tailored for text summarization tasks. BART is a powerful sequence-to-sequence model that combines the strengths of both BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer). BART is designed to handle various NLP tasks by reconstructing original texts from corrupted input sequences. This ability makes it versatile in a range of applications, including text generation, translation, and particularly summarization.\n",
    "8.1.1\tModel Architecture\n",
    "The BART-large model is based on the standard BART architecture, which consists of a Transformer-based encoder-decoder framework. The encoder is responsible for reading the input text and converting it into a sequence of representations, while the decoder generates the output text (in this case, the summary) based on these representations.\n",
    "Encoder: The encoder in BART-large operates in a bidirectional manner, similar to BERT. It processes the entire input sequence, taking into account the context from both the left and right sides of each token. This enables the model to capture the full context of the input text, which is crucial for producing accurate summaries.\n",
    "Decoder: The decoder in BART is autoregressive, like GPT, meaning it generates the summary token by token, with each token being conditioned on the previously generated ones. This allows the model to generate coherent and fluent summaries that are consistent with the input text.\n",
    "Pretraining and Fine-tuning: BART-large is pretrained on a large corpus of text data using a denoising objective, where the model learns to reconstruct the original text from a corrupted version. After pretraining, the model is fine-tuned specifically on the CNN/DailyMail dataset, which is a popular benchmark for summarization tasks. The fine-tuning process tailors the model's capabilities to generate high-quality summaries for news articles.\n",
    "\n",
    "8.1.2\tKey Features and Performance\n",
    "\n",
    "Text Summarization: The BART-large model excels in abstractive summarization, where the summary is generated in natural language, potentially using new phrases that are not present in the original text. This makes it more flexible and capable of producing summaries that are concise yet informative.\n",
    "8.1.2.1\tROUGE Scores \n",
    "The performance of BART-large is often measured using ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, which compare the overlap between the generated summary and reference summaries. BART-large achieves high ROUGE scores on the CNN/DailyMail dataset, making it one of the top-performing models in the field of summarization.\n",
    "8.1.2.2\tModel Size\n",
    " As a \"large\" model, BART-large has 12 encoder layers and 12 decoder layers, with a total of 406 million parameters. This large parameter count allows the model to capture complex patterns in the data, contributing to its strong performance but also requiring significant computational resources for training and inference.\n",
    "8.1.3\tApplications\n",
    "BART-large is widely used for generating summaries of long documents, news articles, and reports. Its ability to produce coherent and concise summaries has made it a popular choice in both academic research and industry applications. It is particularly useful in scenarios where quick and accurate summarization of text is needed, such as in news aggregation services, legal document analysis, and content curation platforms.\n",
    "8.1.4\tConclusion\n",
    "Facebook's BART-large is a state-of-the-art model for abstractive text summarization, leveraging a sophisticated encoder-decoder architecture to produce high-quality summaries. Its effectiveness in capturing the essence of lengthy documents makes it a valuable tool in various natural language processing applications.\n",
    "\n",
    "\n",
    "\n",
    "8.2\tGoogle‚Äôs Pegasus\n",
    "The Pegasus model, developed by Google Research, is a specialized variant of the Pegasus (Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence) architecture, tailored for the task of abstractive text summarization. Pegasus is designed to excel at generating concise and coherent summaries by pretraining the model with a unique objective that prepares it for summarization tasks. The Pegasus model is specifically fine-tuned on the XSum (Extreme Summarization) dataset, which is known for its challenging and concise summaries, often just a single sentence long.\n",
    "'''\n",
    "document = Document(\n",
    "    page_content= docs,\n",
    "    metadata={\"source\": \"https://example.com\"}\n",
    ")\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\tText Summarization Models\n",
      "8.1\tFacebook's BART-large\n",
      "The BART-large model, developed by Facebook AI, is a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture tailored for text summarization tasks. BART is a powerful sequence-to-sequence model that combines the strengths of both BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer). BART is designed to handle various NLP tasks by reconstructing original texts from corrupted input sequences. This ability makes it\n",
      "NLP tasks by reconstructing original texts from corrupted input sequences. This ability makes it versatile in a range of applications, including text generation, translation, and particularly summarization.\n",
      "8.1.1\tModel Architecture\n",
      "The BART-large model is based on the standard BART architecture, which consists of a Transformer-based encoder-decoder framework. The encoder is responsible for reading the input text and converting it into a sequence of representations, while the decoder generates the output text (in this case, the summary) based on these representations.\n",
      "Encoder: The encoder in BART-large operates in a bidirectional manner, similar to BERT. It processes the entire input sequence, taking into account the context from both the left and right sides of each token. This enables the model to capture the full context of the input text, which is crucial for producing accurate summaries.\n",
      "Decoder: The decoder in BART is autoregressive, like GPT, meaning it generates the summary token by token, with each token being conditioned on the previously generated ones. This allows the model to generate coherent and fluent summaries that are consistent with the input text.\n",
      "Pretraining and Fine-tuning: BART-large is pretrained on a large corpus of text data using a denoising objective, where the model learns to reconstruct the original text from a corrupted version. After pretraining, the model is fine-tuned specifically on the CNN/DailyMail dataset, which is a popular benchmark for summarization tasks. The fine-tuning process tailors the model's capabilities to generate high-quality summaries for news articles.\n",
      "8.1.2\tKey Features and Performance\n",
      "Text Summarization: The BART-large model excels in abstractive summarization, where the summary is generated in natural language, potentially using new phrases that are not present in the original text. This makes it more flexible and capable of producing summaries that are concise yet informative.\n",
      "8.1.2.1\tROUGE Scores\n",
      "8.1.2.1\tROUGE Scores \n",
      "The performance of BART-large is often measured using ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, which compare the overlap between the generated summary and reference summaries. BART-large achieves high ROUGE scores on the CNN/DailyMail dataset, making it one of the top-performing models in the field of summarization.\n",
      "8.1.2.2\tModel Size\n",
      "8.1.2.2\tModel Size\n",
      " As a \"large\" model, BART-large has 12 encoder layers and 12 decoder layers, with a total of 406 million parameters. This large parameter count allows the model to capture complex patterns in the data, contributing to its strong performance but also requiring significant computational resources for training and inference.\n",
      "8.1.3\tApplications\n",
      "8.1.3\tApplications\n",
      "BART-large is widely used for generating summaries of long documents, news articles, and reports. Its ability to produce coherent and concise summaries has made it a popular choice in both academic research and industry applications. It is particularly useful in scenarios where quick and accurate summarization of text is needed, such as in news aggregation services, legal document analysis, and content curation platforms.\n",
      "8.1.4\tConclusion\n",
      "8.1.4\tConclusion\n",
      "Facebook's BART-large is a state-of-the-art model for abstractive text summarization, leveraging a sophisticated encoder-decoder architecture to produce high-quality summaries. Its effectiveness in capturing the essence of lengthy documents makes it a valuable tool in various natural language processing applications.\n",
      "8.2\tGoogle‚Äôs Pegasus\n",
      "The Pegasus model, developed by Google Research, is a specialized variant of the Pegasus (Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence) architecture, tailored for the task of abstractive text summarization. Pegasus is designed to excel at generating concise and coherent summaries by pretraining the model with a unique objective that prepares it for summarization tasks. The Pegasus model is specifically fine-tuned on the XSum (Extreme\n",
      "it for summarization tasks. The Pegasus model is specifically fine-tuned on the XSum (Extreme Summarization) dataset, which is known for its challenging and concise summaries, often just a single sentence long.\n"
     ]
    }
   ],
   "source": [
    "# Create a RecursiveCharacterTextSplitter instance\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split the document into chunks\n",
    "# Note: split_documents expects a list of Document objects, not a tuple\n",
    "final_docs = splitter.split_documents([document])\n",
    "\n",
    "# Print the split documents\n",
    "for doc in final_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In addition to its impressive performance, Google's Pegasus stands out for its unique pretraining objective and fine-tuned architecture, specifically designed for abstractive summarization tasks. This includes a special variant of the Pegasus model that is optimized for the challenging and concise summaries found in the XSum dataset. This advanced technology has gained widespread use in various industries and research fields, solidifying BART-large and Pegasus as leaders in natural language processing and pushing the boundaries of what is possible with text summarization.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    Write a concise and short summary of the following speech,\n",
    "    speech: {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "   \n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt_template,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": final_docs}, return_only_outputs=True)\n",
    "\n",
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain \n",
    "from langchain_openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def show_page():\n",
    "    load_dotenv()\n",
    "    os.environ.get(\"OPENAI_API_KEY\")\n",
    "    template = \"\"\"\n",
    "    Write a concise and short summary of the following speech,\n",
    "    speech: {text}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "    refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "   \n",
    "    )\n",
    "    refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "\n",
    "    llm = OpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "    st.title(\"Text Summarizer :robot_face:\")\n",
    "    st.subheader(\" Long Text Summarize \")\n",
    "\n",
    "    doc = st.text_input(\"Enter the Text\")\n",
    "\n",
    "    if st.button(\"Summarize\"):\n",
    "        if not doc.strip():\n",
    "            st.error(\"Please enter a text\")\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                with st.spinner(\"Loading content...\"):\n",
    "                    document = Document(\n",
    "                                        page_content= doc\n",
    "                                        )\n",
    "                    # Create a RecursiveCharacterTextSplitter instance\n",
    "                    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split the document into chunks\n",
    "# Note: split_documents expects a list of Document objects, not a tuple\n",
    "                    final_docs = splitter.split_documents([document])\n",
    "\n",
    "                    if data:\n",
    "                        chain = load_summarize_chain(\n",
    "                                llm=llm,\n",
    "                                chain_type=\"refine\",\n",
    "                                question_prompt=prompt_template,\n",
    "                                refine_prompt=refine_prompt,\n",
    "                                return_intermediate_steps=True,\n",
    "                                input_key=\"input_documents\",\n",
    "                                output_key=\"output_text\",\n",
    "                                )\n",
    "                        result = chain({\"input_documents\": final_docs}, return_only_outputs=True)\n",
    "\n",
    "                        output =result[\"output_text\"]\n",
    "                        st.success(output)\n",
    "                    else:\n",
    "                        st.error(\"Failed to load content from the text.\")\n",
    "            except Exception as e:\n",
    "                st.exception(f\"Exception: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
